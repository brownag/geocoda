---
title: "Gap Filling & Edge Handling"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Gap Filling & Edge Handling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

## Overview

Handle sparse data, edge effects, and detection limits through bootstrap resampling, edge extension, and zero imputation.

---

## Workflow

### 1. Bootstrap Resampling

Expand sparse datasets by generating plausible compositional variants at same locations:

```{r setup_and_resample, include=TRUE}
library(geocoda)
library(terra)
library(sf)
library(gstat)
library(compositions)

# Sparse observations
sparse_obs <- data.frame(
  x = c(10, 30, 50, 70, 90),
  y = c(20, 40, 60, 80, 85),
  SAND = c(55, 48, 35, 42, 60),
  SILT = c(30, 38, 45, 40, 25),
  CLAY = c(15, 14, 20, 18, 15)
)

# Expand from 5 to 15 samples
resampled <- gc_resample_compositions(
  composition_grid = sparse_obs[, c("SAND", "SILT", "CLAY")],
  n = 15,
  method = "uniform"
)

# Replicate spatial coordinates, use resampled compositions
expanded_spatial <- do.call(rbind, replicate(3, sparse_obs, simplify = FALSE))
expanded_spatial$SAND <- as.numeric(resampled$samples[, "SAND"])
expanded_spatial$SILT <- as.numeric(resampled$samples[, "SILT"])
expanded_spatial$CLAY <- as.numeric(resampled$samples[, "CLAY"])

nrow(expanded_spatial)
```

### 2. Edge Extension

Add mirrored samples beyond boundaries to reduce edge effects:

```{r edge_extension}
extend_at_edges <- function(data, extend_factor = 0.2) {
  x_range <- diff(range(data$x))
  y_range <- diff(range(data$y))
  extend_x <- extend_factor * x_range
  extend_y <- extend_factor * y_range
  
  extended_left <- data.frame(
    x = data$x - 2 * extend_x, y = data$y,
    data[, c("SAND", "SILT", "CLAY")]
  )
  extended_right <- data.frame(
    x = data$x + 2 * extend_x, y = data$y,
    data[, c("SAND", "SILT", "CLAY")]
  )
  extended_bottom <- data.frame(
    x = data$x, y = data$y - 2 * extend_y,
    data[, c("SAND", "SILT", "CLAY")]
  )
  extended_top <- data.frame(
    x = data$x, y = data$y + 2 * extend_y,
    data[, c("SAND", "SILT", "CLAY")]
  )
  
  rbind(data[, c("x", "y", "SAND", "SILT", "CLAY")],
        extended_left, extended_right, extended_bottom, extended_top)
}

extended <- extend_at_edges(expanded_spatial, extend_factor = 0.25)
nrow(extended)  # 5 original + 20 from 4 directions
```

### 3. Handle Zero Values

Impute values below detection limit:

```{r zero_handling}
data_with_zeros <- data.frame(
  x = c(1, 2, 3, 4, 5),
  y = c(1, 2, 3, 4, 5),
  SAND = c(50, 48, 0, 42, 60),
  SILT = c(30, 38, 0, 40, 25),
  CLAY = c(20, 14, 100, 18, 15)
)

handled_zeros <- gc_handle_zeros(
  comp_data = data_with_zeros[, c("SAND", "SILT", "CLAY")],
  dl = 0.5,
  method = "mzero"
)

head(handled_zeros)
```

### 4. Fit Variogram

```{r variogram}
# Use gap-filled data
comp_data <- extended[, c("SAND", "SILT", "CLAY")]
ilr_params <- gc_ilr_params(comp_data)

ilr_transformed <- as.data.frame(
  compositions::ilr(compositions::acomp(comp_data))
)
colnames(ilr_transformed) <- paste0("ilr", seq_len(ncol(ilr_transformed)))
vgm_data <- cbind(extended[, c("x", "y")], ilr_transformed)

vgm_fitted <- gc_fit_vgm(
  ilr_params,
  data = vgm_data,
  aggregate = TRUE
)

vgm_fitted$map1$psill
vgm_fitted$map1$range
```

### 5. Simulate

```{r simulate}
pred_grid <- expand.grid(
  x = seq(0, 100, by = 10),
  y = seq(0, 100, by = 10)
)
pred_grid_sf <- st_as_sf(pred_grid, coords = c("x", "y"), crs = NA)

model <- gc_ilr_model(ilr_params, vgm_fitted)

sims <- gc_sim_composition(
  model = model,
  locations = pred_grid_sf,
  nsim = 3,
  target_names = c("SAND", "SILT", "CLAY")
)

names(sims)
```

---

## Clustering Down-Weighting

Down-weight samples in dense clusters:

```{r clustering}
cluster_weights <- function(coords, cluster_radius = 10) {
  weights <- rep(1, nrow(coords))
  for (i in seq_len(nrow(coords))) {
    distances <- sqrt((coords$x - coords$x[i])^2 + 
                      (coords$y - coords$y[i])^2)
    n_neighbors <- sum(distances < cluster_radius & distances > 0)
    if (n_neighbors > 2) weights[i] <- 1 / (n_neighbors + 1)
  }
  weights * length(weights) / sum(weights)
}

sample_coords <- data.frame(
  x = sample(0:100, 10, replace = FALSE),
  y = sample(0:100, 10, replace = FALSE)
)

weights <- round(cluster_weights(sample_coords, cluster_radius = 15), 2)
cbind(sample_coords, weight = weights)
```

---

## Decision Guide

- **Zeros/detection limits** → Use `gc_handle_zeros()`
- **Few samples** (<10) → Use `gc_resample_compositions()`
- **Clustered data** → Apply distance-based weights
- **Edge effects** → Use `extend_at_edges()` function

---

## Key points

[OK] Bootstrap resampling expands sparse datasets
[OK] Edge extension reduces boundary artifacts
[OK] Zero handling prevents compositional errors
[OK] Clustering weights improve variogram stability

---

## References

- `?gc_resample_compositions()` -- Bootstrap expansion
- `?gc_handle_zeros()` -- Detection limit imputation
- `?gc_fit_vgm()` -- Variogram fitting
