---
title: "FAQ"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{FAQ}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", eval = FALSE)
```

## When to Use geocoda

**Use for:** Compositional data (soil texture, geochemistry) requiring multiple maps with guaranteed constraints.

**Don't use for:** Non-compositional data, binary/categorical, or deterministic relationships.

---

## What is Compositional Data?

Compositional data sum to a whole: sand + silt + clay = 100%. Standard statistics fail because:
- Cannot exceed 100%
- Components are dependent (if one increases, others decrease)
- Zero values problematic

geocoda uses **Isometric Log-Ratio (ILR)** transforms:
1. Convert constrained (sand, silt, clay) → unconstrained (ilr1, ilr2)
2. Do standard geostatistics
3. Back-transform with constraints guaranteed

---

## geocoda vs Standard Kriging

| Feature | Standard | geocoda |
|---------|----------|---------|
| Constraints | [NO] | [OK] |
| Ensemble uncertainty | [MAYBE] | [OK] |
| Sum guaranteed | [NO] | [OK] |
| Multi-source | [MAYBE] | [OK] |
| Zone pooling | [NO] | [OK] |

---

## Do I Need All Workflows?

No. Choose what you need:

- **Simulation only:** Basic conditional simulation
- **Simulation + Ensemble:** Uncertainty maps
- **Simulation + Hierarchical:** Regional zones
- **Simulation + Data integration + Ensemble:** SSURGO + field data
- **All workflows:** Complete analysis

---

## What is a Realization?

A **realization** is one equally-likely spatial map that:
- Respects the variogram (spatial correlation)
- Satisfies constraints (sum to 100%)
- Is consistent with model assumptions

Think: One-among-many possible plausible maps.

---

## How Many Realizations?

A pragmatic guide:

- **Publication maps:** 50-100 realizations (smooth mean estimates)
- **Uncertainty quantification:** 20-30 (95% CI/PI robust)
- **Testing:** 10-15 (quick exploration)
- **Site-specific:** 100-500 (detailed uncertainty at points)

Rule: More realizations = more stable estimates but longer computation.

---

## How to Choose Variogram Parameters

**Range:** Domain diagonal / 3 (standard start point)
- Fit empirically with `gc_fit_vgm()`
- Scale 0.5x-2.0x based on domain characteristics

**Nugget:** 10-30% of sill (measurement noise)

**Psill:** Data variance (rarely adjusted)

See `vignette("parameter_selection")` for details.

---

## Prediction Intervals vs Confidence Intervals

- **Prediction Interval (95% PI):** Range of likely individual realized values (wider)
- **Confidence Interval (95% CI):** Range of mean prediction (narrower)

geocoda outputs <strong>PI</strong> (useful for site-specific uncertainty).

---

## Handling Sparse Data

- **<5 pits:** Use `gc_resample_compositions()` bootstrap
- **5-15 pits:** Pool information via `gc_resample_compositions()`
- **>15 pits:** Standard approaches work well

See `vignette("gap_filling")` for details.

---

## Common Errors

| Error | Cause | Fix |
|-------|-------|-----|
| "Sum != 100%" | Data validation | Use `gc_handle_zeros()` |
| "Singular matrix" | Insufficient samples | Increase `nsim` or use defaults |
| "Range too large" | Parameter miscalibration | Scale 0.5x-1.5x from default |
| "No convergence" | Weak spatial pattern | Check data quality |

---

## See Also

- `vignette("getting_started")` -- Entry point
- `vignette("parameter_selection")` -- Variogram tuning
- `vignette("gap_filling")` -- Sparse data handling
Realization 2: 47%, 42%, 45%, 41%, ... another equally plausible guess
Realization 3: 43%, 46%, 44%, 45%, ... a third option
...
```

The **spread across realizations** represents your **uncertainty**.

---

### Q: How many realizations should I generate?

**A:** 
- **Quick assessment:** 5–10 realizations (fast, gives sense of uncertainty)
- **Publication/decisions:** 20–50 realizations (good confidence in percentiles)
- **Extreme value analysis:** 100+ realizations (for rare events like >80% clay)
- **Memory limited:** Aggregate down to mean + SD (captures 90% of info in 5% storage)

**Rule of thumb:** More realizations = slower + more storage, but better uncertainty estimates.

```r
# Fast mode
sims_fast <- gc_sim_composition(model, grid, nsim = 5)

# Thorough analysis
sims_thorough <- gc_sim_composition(model, grid, nsim = 50)

# Memory-conscious: Get stats directly without storing all realizations
stats <- gc_aggregate_realizations(sims_thorough)  # keep only mean, sd, quantiles
```

---

## Parameter Questions

### Q: How do I choose variogram parameters?

**A:** Three approaches, in order of preference:

**1. Fit from observed data (recommended):**
```r
# If you have field samples at actual locations:
fitted_vgm <- gc_fit_vgm(ilr_params, data = your_samples, aggregate = TRUE)
model <- gc_ilr_model(ilr_params, variogram_model = fitted_vgm)
```

**2. Use automatic suggestions:**
```r
default_vgm <- gc_vgm_defaults(ilr_params)
# Returns: typical range, psill, nugget for soil data
```

**3. Manual specification (expert input):**
```r
vgm_model <- vgm(
  psill = 0.4,      # Typical soil: 0.3–0.6
  model = "Exp",    # Exponential (most common)
  range = 50,       # 20–100 m typical for soil
  nugget = 0.1      # Measurement error
)
```

**Parameters explained:**
- **psill** (partial sill): How different are soils at far distances?
- **range**: Distance at which spatial correlation becomes negligible
- **nugget**: Measurement error or microscale variability
- **model**: Shape of correlation decay (Exponential, Spherical, Gaussian)

---

### Q: How do I set tight vs loose constraints?

**A:** Use `gc_expand_bounds()` `tolerance` parameter:

```r
# TIGHT constraints (realistic soil textures only)
tight_grid <- gc_expand_bounds(constraints, step = 1, tolerance = 1.5)

# LOOSE constraints (broader range, includes edge cases)
loose_grid <- gc_expand_bounds(constraints, step = 1, tolerance = 5.0)
```

**Trade-off:**
- Tight: More realistic, fewer realizations pass constraints
- Loose: More flexible, includes non-typical soils
- **Recommended:** Use tight constraints based on field experience

---

### Q: What does the "pooling" parameter do in hierarchical models?

**A:** In hierarchical shrinkage models, **pooling** controls how much zones **borrow information** from each other:

```r
priors <- gc_set_hierarchy_priors(
  hierarchy,
  pooling = "none"        # Independent zones (no sharing)
)
# vs
priors <- gc_set_hierarchy_priors(
  hierarchy,
  pooling = "strong"      # Heavy sharing (zones pull toward global mean)
)
```

**When to use each:**
- **"none"**: Each zone has distinct soil properties (different parent material)
- **"weak"**: Slight regional effects but mostly independent zones
- **"moderate"**: Similar zones but zone-specific adjustments needed (recommended)
- **"strong"**: Very similar zones, want to combine information heavily

**Visual analogy:**
```
"none":       [Zone A: 40±5] [Zone B: 45±5]  ← Separate estimates
"moderate":  [Zone A: 41±4] [Zone B: 44±4]  ← Slightly pulled together
"strong":    [Zone A: 42±3] [Zone B: 43±3]  ← Heavily pooled
```

---

### Q: Is hierarchical modeling true Bayesian MCMC?

**A:** No, not in v0.2.x. The hierarchical zone functions use **empirical Bayes shrinkage estimation**, which is:

**Fast analytical method (what you have now):**
- ✓ Analytical solution (no sampling needed)
- ✓ Results in seconds
- ✓ Zone estimates + shrinkage weights
- ✗ No posterior samples
- ✗ No Bayesian uncertainty in the formal sense

**True Bayesian MCMC (planned for v0.3.0):**
- ✓ Full posterior distributions
- ✓ Formal Bayesian inference
- ✓ Posterior predictive checks
- ✓ Model comparison (WAIC, LOO)
- ✗ Slower (minutes instead of seconds)

**When to upgrade to MCMC (v0.3.0+):**
- Publishing peer-reviewed research
- Formal Bayesian inference required
- Need full uncertainty quantification
- Small datasets where shrinkage assumptions matter

**For now (v0.2.x), shrinkage is recommended for:**
- Exploration and visualization
- Quick iterations
- Large regional surveys
- When computational speed matters

See `?gc_fit_hierarchical_model()` for details on the current analytical method.

---

## Data & Integration Questions

### Q: Can I condition simulations on field observations?

**A:** Yes! Pass observed data when building the model:

```r
# Prepare observed data in ILR space
obs_ilr <- compositions::ilr(compositions::acomp(obs_textures))
obs_data <- sf::st_as_sf(
  cbind(obs_locations, as.data.frame(obs_ilr)),
  coords = c("x", "y")
)

# Build conditioned model
model <- gc_ilr_model(
  ilr_params,
  variogram_model = vgm_model,
  data = obs_data  # Pass observations
)

# Simulations now honor observed values exactly at sample sites
sims <- gc_sim_composition(model, grid, nsim = 10)
```

**Result:** Realizations pass exactly through observed points, uncertainty decreases away from observations.

---

### Q: How do I use SSURGO data?

**A:** Two approaches: direct live queries or integration with field data.

**Direct query from SDA:**
```r
# Query live SSURGO database by extent
ssurgo_data <- gc_prepare_ssurgo_direct(
  extent = your_study_area,  # sf/sp object or bbox
  depth_slices = c("0-10", "10-30", "30-60"),
  use_cache = TRUE
)

# Result: Data frame with sand/silt/clay/ilr1/ilr2 by depth
```

**Integration with field data:**
```r
# Weight SSURGO + field observations
combined <- gc_weight_components(
  ssurgo_data = ssurgo,
  field_data = field_samples,
  weights = c(0.3, 0.7)  # 30% SSURGO, 70% field
)
```

---

### Q: How do I handle zero values in compositions?

**A:** Zero values cause problems in log-ratio transforms. geocoda provides solutions:

```r
# Detection limit imputation
no_zeros <- gc_handle_zeros(
  data = raw_textures,
  method = "impute",        # Replace zeros with small value
  detection_limit = 0.5     # Replace <0.5% with 0.5%
)

# Additive log-ratio (alternative transform)
# Already handled automatically by gc_ilr_params()
```

**Common scenarios:**
- **No clay detected** → Use detection limit (0.5% clay)
- **Systematic zeros** → May indicate classification/measurement issue
- **Rare zeros** → Imputation usually sufficient

---

## Performance & Scalability Questions

### Q: My prediction grid is huge (1M+ points). What do I do?

**A:** Use chunking to process in manageable batches:

```r
# Process 10,000 points at a time (reduces memory peak by 80%)
sims <- gc_sim_composition(
  model, 
  huge_grid,
  nsim = 10,
  chunk_size = 10000  # Process in 10k-point chunks
)

# Or save each realization separately to disk
for (i in seq(50)) {
  sim_i <- gc_sim_composition(model, huge_grid, nsim = 1)
  terra::writeRaster(sim_i, paste0("sim_", i, ".tif"))
}
```

---

### Q: How can I speed up kriging?

**A:** 
1. **Reduce prediction grid size** if possible
2. **Use univariate kringing** (default, faster than LMC)
3. **Enable parallelization** for multiple realizations:
   ```r
   future::plan(multisession)  # Use all CPU cores
   sims_parallel <- gc_sim_composition(model, grid, nsim = 50)
   ```
4. **Aggregate realizations** instead of storing individual maps

Typical speedup: 2–4× on quad-core systems.

---

## Validation & Diagnostics Questions

### Q: How do I know my model is good?

**A:** Use diagnostic functions:

```r
# Cross-validation: Leave-one-out error
cv_results <- gc_cross_validate(data, n_folds = 5)
print(cv_results$rmse_per_component)  # Should be <5% of mean

# Uncertainty quantification check
entropy <- gc_compute_entropy(sims)
plot(entropy)  # Should increase away from sample locations

# Constraint satisfaction
constraint_check <- gc_validate_realizations(sims, data)
print(constraint_check$percent_valid)  # Should be >99%

# Comprehensive report
report <- gc_ensemble_quality_report(sims, data, hierarchy)
# Generates: quality_report.html with all diagnostics
```

---

### Q: What does RMSE mean for soil texture?

**A:** RMSE (Root Mean Squared Error) = typical prediction error.

Example interpretation:
- **RMSE = 3%** → On average, predictions off by ±3% (good!)
- **RMSE = 10%** → Predictions off by ±10% (acceptable for exploratory)
- **RMSE = 20%** → Predictions off by ±20% (use with caution)

**For soil texture:** RMSE < 5% is excellent, < 10% is acceptable.

---

### Q: How do I interpret uncertainty maps?

**A:** High uncertainty means "I'm really unsure about this location":

```r
# Create uncertainty map (standard deviation)
std_dev <- gc_aggregate_realizations(sims, statistics = "sd")

# High std dev areas:
# - Few or no nearby samples
# - High variability in observed data
# - Far from training data

# Low std dev areas:
# - Dense sampling
# - Low variability in data  
# - Near observed points (if conditioned)
```

Use uncertainty maps to guide **future sampling** (add samples where uncertainty is high).

---

## Troubleshooting & Error Messages

### Q: "Error: Sum constraint violated in realizations"

**A:** Usually indicates:
1. **Tight tolerance on constraints** → Relax slightly
2. **Back-transform numerical error** → Try alternative solver
3. **Malformed variogram** → Verify psill, range, nugget values

Fix:
```r
# Check constraint settings
constraints <- list(
  SAND = list(min = 5, max = 80),
  SILT = list(min = 0, max = 75),
  CLAY = list(min = 0, max = 60)
)

# Verify sum
gc_validate_input_data(your_data, component_names = c("SAND", "SILT", "CLAY"))
```

---

### Q: "Warning: Some realizations did not converge"

**A:** Kriging failed to reach solution. Usually harmless but check:

```r
# Verify variogram parameters are reasonable
plot(empirical_vgm)
plot(vgm_model, empirical_vgm)

# Try simplified model
vgm_simple <- vgm(psill = 0.5, model = "Sph", range = 50)
model <- gc_ilr_model(ilr_params, vgm_simple)
```

---

### Q: "Memory limit exceeded"

**A:** Reduce ensemble or grid size:

```r
# Instead of 100 realizations:
sims_small <- gc_sim_composition(model, grid, nsim = 10)

# Process in chunks:
sims_chunked <- gc_sim_composition(model, grid, nsim = 100, chunk_size = 5000)

# Or sample prediction grid:
grid_sample <- grid[sample(nrow(grid), 1000), ]
sims_sample <- gc_sim_composition(model, grid_sample, nsim = 50)
```

---

## When to Contact Support

Report bugs or ask questions if:
- Error message doesn't match FAQ solutions
- Unexpected results despite following documentation
- Feature request for new functionality
- Performance issues with specific data types

Visit: [github.com/brownag/geocoda/issues](https://github.com/brownag/geocoda/issues)

---

## Additional Resources

- **Compositional data theory:** Aitchison (1986), Egozcue et al. (2003)
- **Geostatistics:** Chilès & Delfiner (2012), Diggle & Ribeiro (2007)
- **Soil composition:** USDA Soil Texture Triangle classification

See `vignette("references")` for full citations.
